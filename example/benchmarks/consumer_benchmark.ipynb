{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to End Benchmark: Producer (Dask) - Kafka Cluster - Consumer (Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Libraries\n",
    "import sys, os\n",
    "sys.path.insert(0, \"..\")\n",
    "import pandas as pd\n",
    "import uuid\n",
    "## logging\n",
    "import logging\n",
    "import time\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"radical.utils\").setLevel(logging.ERROR)\n",
    " \n",
    "# Pilot-Streaming\n",
    "import pilot.streaming\n",
    "import masa.spark\n",
    "import mass.kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Setup resources for test\n",
    "2. Start Producer Mini App in Dask producing synthetic data\n",
    "3. Start Consumer Mini App in Spark for processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_repeats in range(5):\n",
    "    for num_producer_nodes in [1]:\n",
    "        for num_broker_nodes in [1,2,4]:\n",
    "            for number_spark_nodes in [1,2,4,8,16]:\n",
    "                for application in [\"light-mlem\", \"light-gridrec\", \"kmeansstaticpred-1000\",   \"kmeansstatic-1000\"]:\n",
    "                    #num_broker_nodes=1\n",
    "                    #num_producer_nodes=1\n",
    "                    #number_spark_nodes=1\n",
    "                    run_id = str(uuid.uuid1())\n",
    "                    topic_name = \"test_\" + run_id\n",
    "                    number_parallel_tasks=8\n",
    "                    kafka_pilot_description1 = {\n",
    "                                        \"resource\":\"slurm+ssh://login1.wrangler.tacc.utexas.edu\",\n",
    "                                        \"working_directory\": os.path.join('/work/01131/tg804093/wrangler/', \"work\"),\n",
    "                                        \"number_cores\": 48*num_broker_nodes,\n",
    "                                        \"project\": \"TG-MCB090174\",\n",
    "                                        \"queue\": \"normal\",\n",
    "                                        \"walltime\": 159,\n",
    "                                        \"type\":\"kafka\"\n",
    "                                    }\n",
    "                    kafka_pilot = pilot.streaming.PilotComputeService.create_pilot(kafka_pilot_description1)\n",
    "                    kafka_pilot.wait()\n",
    "                    \n",
    "                    dask_pilot_description = {\n",
    "                        \"resource\":\"slurm+ssh://login1.wrangler.tacc.utexas.edu\",\n",
    "                        \"working_directory\": os.path.join('/work/01131/tg804093/wrangler/', \"work\"),\n",
    "                        \"number_cores\": 48*num_producer_nodes,\n",
    "                        \"project\": \"TG-MCB090174\",\n",
    "                        \"queue\": \"normal\",\n",
    "                        \"walltime\": 159,\n",
    "                        \"type\":\"dask\"\n",
    "                    }\n",
    "                    dask_pilot = pilot.streaming.PilotComputeService.create_pilot(dask_pilot_description)\n",
    "                    dask_pilot.wait()\n",
    "                    \n",
    "                    spark_pilot_description = {\n",
    "                        \"resource\":\"slurm+ssh://login1.wrangler.tacc.utexas.edu\",\n",
    "                        \"working_directory\": os.path.join('/work/01131/tg804093/wrangler/', \"work\"),\n",
    "                        \"number_cores\": 48*number_spark_nodes,\n",
    "                        \"project\": \"TG-MCB090174\",\n",
    "                        \"queue\": \"normal\",\n",
    "                        \"walltime\": 159,\n",
    "                        \"type\":\"spark\"\n",
    "                    }\n",
    "                    spark_pilot = pilot.streaming.PilotComputeService.create_pilot(spark_pilot_description)\n",
    "                    spark_pilot.wait()\n",
    "                \n",
    "                    \n",
    "                    number_clusters = 100\n",
    "                    if application.startswith(\"kmeans\") and application.find(\"-\")>=0:\n",
    "                        number_clusters = int(application.split(\"-\")[1])\n",
    "                        application = application.split(\"-\")[0]\n",
    "                     \n",
    "                    produce_interval=0\n",
    "                    if application.startswith(\"light\"): produce_interval=10\n",
    "                        \n",
    "                    print \"Application: %s, Number Clusters: %d\"%(application, number_clusters)\n",
    "                        \n",
    "                    # Scenario: \n",
    "                    prod=mass.kafka.MiniApp(\n",
    "                                               dask_scheduler=dask_pilot.get_details()['master_url'],\n",
    "                                               kafka_zk_hosts=kafka_pilot.get_details()[\"master_url\"],\n",
    "                                               number_parallel_tasks=number_parallel_tasks,\n",
    "                                               number_clusters=192, # kmeans\n",
    "                                               number_points_per_cluster=52084, # kmeans\n",
    "                                               number_points_per_message=5000, # kmeans\n",
    "                                               number_dim=3, # kmeans\n",
    "                                               number_messages=400, # light\n",
    "                                               number_produces=50,\n",
    "                                               number_partitions=num_broker_nodes*12,\n",
    "                                               topic_name=topic_name,\n",
    "                                               application_type=application,\n",
    "                                               produce_interval=produce_interval\n",
    "                                            )\n",
    "                    prod.run_in_background()\n",
    "    \n",
    "                    consumer = masa.spark.MiniApp(\n",
    "                                              spark_master=spark_pilot.get_details()[\"master_url\"],\n",
    "                                              kafka_zk_hosts=kafka_pilot.get_details()[\"master_url\"],\n",
    "                                              topic_name = topic_name,\n",
    "                                              number_clusters=number_clusters,\n",
    "                                              test_scenario=\"%s-%d-%d-%d-%d\"%(application,num_producer_nodes, num_broker_nodes, number_spark_nodes, number_clusters),\n",
    "                                              application = application\n",
    "                                             )\n",
    "                    consumer.run_in_background()\n",
    "                    \n",
    "                    # Wait for completion\n",
    "                    prod.wait()\n",
    "                    time.sleep(240)\n",
    "                    print \"******** Producer Wait RETURNED. Cancel Streaming App\"\n",
    "                    consumer.cancel()\n",
    "                    \n",
    "                    kafka_pilot.cancel()\n",
    "                    dask_pilot.cancel()\n",
    "                    spark_pilot.cancel() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to executable script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script consumer_benchmark.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh login1.wrangler.tacc.utexas.edu sbatch `pwd`/submit.slurm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
