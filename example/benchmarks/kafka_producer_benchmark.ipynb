{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Producer Mini-App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# System Libraries\n",
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pykafka\n",
    "import mass.kafka\n",
    " \n",
    "\n",
    "## logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"tornado.application\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"distributed.utils\").setLevel(logging.CRITICAL)\n",
    "\n",
    "\n",
    "# Pilot-Streaming\n",
    "import pilot.streaming\n",
    "import uuid \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Benchmark Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpkaz188w9\n",
      "Submission of Job Command: ssh login1.wrangler.tacc.utexas.edu sbatch  tmpkaz188w9\n",
      "Cleanup: ssh login1.wrangler.tacc.utexas.edu rm tmpkaz188w9\n",
      "**** Job: 105894 State : Running\n",
      "/tmp/tmp24icjacu\n",
      "Submission of Job Command: ssh login1.wrangler.tacc.utexas.edu sbatch  tmp24icjacu\n",
      "Cleanup: ssh login1.wrangler.tacc.utexas.edu rm tmp24icjacu\n",
      "**** Job: 105895 State : Running\n",
      "look for configs in: /work/01131/tg804093/wrangler/work/kafka-fe54cbf4-0236-11e9-a8d3-0b861b3f4538/config\n",
      "['broker-0']\n",
      "Kafka Config: /work/01131/tg804093/wrangler/work/kafka-fe54cbf4-0236-11e9-a8d3-0b861b3f4538/config (Mon Dec 17 14:05:03 2018)\n",
      "{'broker.id': '0', 'listeners': 'PLAINTEXT://c251-119:9092', 'zookeeper.connect': 'c251-119:2181', 'zookeeper.connection.timeout.ms': '6000'}\n",
      "look for configs in: /work/01131/tg804093/wrangler/work/kafka-fe54cbf4-0236-11e9-a8d3-0b861b3f4538/config\n",
      "['broker-0']\n",
      "Kafka Config: /work/01131/tg804093/wrangler/work/kafka-fe54cbf4-0236-11e9-a8d3-0b861b3f4538/config (Mon Dec 17 14:05:03 2018)\n",
      "{'broker.id': '0', 'listeners': 'PLAINTEXT://c251-119:9092', 'zookeeper.connect': 'c251-119:2181', 'zookeeper.connection.timeout.ms': '6000'}\n",
      "Run Application: light, Number Points per Messages: 1\n",
      "Number Messages: 192\n",
      "MASS - Produce to Kafka\n",
      "Use Dask Distributed\n",
      "{'type': 'Scheduler', 'id': 'Scheduler-ad3166ad-1162-4a24-bbf5-26d8f60c284f', 'address': 'tcp://129.114.58.117:8786', 'services': {'bokeh': 8787}, 'workers': {'tcp://129.114.58.117:33175': {'type': 'Worker', 'id': 'tcp://129.114.58.117:33175', 'host': '129.114.58.117', 'resources': {}, 'local_directory': '/home/01131/tg804093/worker-5qgdgm5m', 'name': 'tcp://129.114.58.117:33175', 'ncores': 48, 'memory_limit': 134775570432, 'last_seen': 1545077125.8206234, 'services': {'nanny': 42940, 'bokeh': 33932}, 'metrics': {'cpu': 2.0, 'memory': 81747968, 'time': 1545077125.3201678, 'read_bytes': 29025.952046310755, 'write_bytes': 29914.786939003316, 'num_fds': 25, 'executing': 0, 'in_memory': 0, 'ready': 0, 'in_flight': 0}}}}\n",
      "Kafka/Kinesis: c251-119:2181, Dask: tcp://129.114.58.117:8786, Number Dask Nodes: 1,  Number Parallel Producers: 1\n",
      "/home/01131/tg804093/work/kafka_2.11-2.1.0/bin/kafka-topics.sh --delete --zookeeper c251-119:2181 --topic test-17b745c2-0237-11e9-a8d3-0b861b3f4538\n",
      "/home/01131/tg804093/work/kafka_2.11-2.1.0/bin/kafka-topics.sh --create --zookeeper c251-119:2181 --replication-factor 1 --partitions 1 --topic test-17b745c2-0237-11e9-a8d3-0b861b3f4538\n",
      "/home/01131/tg804093/work/kafka_2.11-2.1.0/bin/kafka-topics.sh --describe --zookeeper c251-119:2181 --topic test-17b745c2-0237-11e9-a8d3-0b861b3f4538\n",
      "Application: light, Broker: kafka, Number Partitions: 1, Generate Block ID: 0/0\n",
      "Waiting for Dask Tasks to complete\n"
     ]
    }
   ],
   "source": [
    "for num_repeats in range(1):\n",
    "    for num_producer_nodes in [1]:\n",
    "        \n",
    "        dask_pilot_description = {\n",
    "                 \"resource\":\"slurm+ssh://login1.wrangler.tacc.utexas.edu\",\n",
    "                 \"working_directory\": os.path.join('/work/01131/tg804093/wrangler/', \"work\"),\n",
    "                 \"number_cores\": 48*num_producer_nodes,\n",
    "                 \"project\": \"TG-MCB090174\",\n",
    "                 \"queue\": \"normal\",\n",
    "                 \"walltime\": 300,\n",
    "                 \"type\":\"dask\"\n",
    "            }\n",
    "        dask_pilot = pilot.streaming.PilotComputeService.create_pilot(dask_pilot_description)\n",
    "        dask_pilot.wait()\n",
    "        dask_details=dask_pilot.get_details()\n",
    "        #dask_details={'master_url': 'tcp://c251-101:8786'}\n",
    "        \n",
    "        for num_broker_nodes in [1]: #,2,4\n",
    "            for num_partitions_per_node in [1,2,4,8,16]:\n",
    "                #for application in [\"kmeans-5000\", \"kmeansstatic-5000\", \"kmeansstatic-10000\", \"kmeansstatic-20000\", \"light\"]:\n",
    "                for application_scenario in [\"light\"]:    #\"kmeans-5000\",\n",
    "                    kafka_pilot_description1 = {\n",
    "                        \"resource\":\"slurm+ssh://login1.wrangler.tacc.utexas.edu\",\n",
    "                        \"working_directory\": os.path.join('/work/01131/tg804093/wrangler/', \"work\"),\n",
    "                        \"number_cores\": 48*num_broker_nodes,\n",
    "                        \"project\": \"TG-MCB090174\",\n",
    "                        \"queue\": \"normal\",\n",
    "                        \"walltime\": 300,\n",
    "                        \"type\":\"kafka\"\n",
    "                    }\n",
    "                    kafka_pilot = pilot.streaming.PilotComputeService.create_pilot(kafka_pilot_description1)\n",
    "                    kafka_pilot.wait()\n",
    "                    kafka_details = kafka_pilot.get_details()\n",
    "                    \n",
    "                    time.sleep(5)\n",
    "                    for number_parallel_tasks in [1,2,4,8,16,32,48]:\n",
    "                        number_parallel_tasks = num_producer_nodes*number_parallel_tasks\n",
    "                        number_points_per_message = 15000\n",
    "                        if application_scenario.startswith(\"kmeans\"):\n",
    "                            number_points_per_message = int(application_scenario.split(\"-\")[1])\n",
    "                            application = application_scenario.split(\"-\")[0]\n",
    "                        else:\n",
    "                            application = application_scenario\n",
    "                            number_points_per_message = 1\n",
    "                        \n",
    "                        print(\"Run Application: %s, Number Points per Messages: %d\"%(application, number_points_per_message))\n",
    "                        run_id = str(uuid.uuid1())\n",
    "                        miniapp=mass.kafka.MiniApp(\n",
    "                                                     dask_scheduler=dask_details['master_url'],\n",
    "                                                     resource_url=kafka_details[\"master_url\"],\n",
    "                                                     broker_service=\"kafka\",\n",
    "                                                     number_parallel_tasks=number_parallel_tasks,\n",
    "                                                     number_clusters=10, # kmeans\n",
    "                                                     number_points_per_cluster=10000, # kmeans\n",
    "                                                     number_points_per_message=number_points_per_message, # kmeans\n",
    "                                                     number_dim=3, # kmeans\n",
    "                                                     number_messages=192, # light\n",
    "                                                     number_produces=1,\n",
    "                                                     number_partitions=num_broker_nodes*num_partitions_per_node,\n",
    "                                                     topic_name=\"test-\"+run_id,\n",
    "                                                     application_type = application\n",
    "                                                    )\n",
    "                        miniapp.run()\n",
    "                    try:\n",
    "                        kafka_pilot.cancel()\n",
    "                        \n",
    "                    except:\n",
    "                        pass\n",
    "        try:\n",
    "            dask_pilot.cancel()\n",
    "            #dask_pilot2.cancel()\n",
    "            time.sleep(60)\n",
    "        except: \n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_pilot.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distributed\n",
    "c=distributed.Client(\"tcp://c251-101:8786\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.scheduler_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc(x):\n",
    "    import socket\n",
    "    return socket.gethostname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.delayed import delayed\n",
    "t = delayed(inc)(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Mini App Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Kafka Broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pykafka.KafkaClient(zookeeper_hosts=kafka_details[\"master_url\"])\n",
    "topic = client.topics['test']\n",
    "producer = topic.get_sync_producer()\n",
    "consumer = topic.get_simple_consumer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "number_total_points = 0\n",
    "number_dimensions = 0\n",
    "for i in range(100):\n",
    "    message = consumer.consume(block=False)\n",
    "    if message is not None:\n",
    "        data_np = np.array(ast.literal_eval(message.value))\n",
    "        num_points = data_np.shape[0]\n",
    "        number_dimensions = data_np.shape[1]\n",
    "        count =  count + 1\n",
    "        number_total_points = number_total_points + num_points\n",
    "    #print \"Consumed message: %d, Number Points: %d, Number Dimensions: %d\"%\\\n",
    "    #        (count, num_points, number_dimensions)   \n",
    "        \n",
    "print(\"Total Messages: %d, Total Points: %d, Number Dimensions: %d\"%(count, number_total_points, number_dimensions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
