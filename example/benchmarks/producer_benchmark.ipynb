{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Producer Mini-App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# System Libraries\n",
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pykafka\n",
    "import mass.kafka\n",
    " \n",
    "\n",
    "## logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"tornado.application\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"distributed.utils\").setLevel(logging.CRITICAL)\n",
    "\n",
    "\n",
    "# Pilot-Streaming\n",
    "import pilot.streaming\n",
    "import uuid \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Benchmark Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_repeats in range(1):\n",
    "    for num_producer_nodes in [24]:\n",
    "        dask_pilot_description = {\n",
    "                 \"resource\":\"slurm+ssh://login1.wrangler.tacc.utexas.edu\",\n",
    "                 \"working_directory\": os.path.join('/work/01131/tg804093/wrangler/', \"work\"),\n",
    "                 \"number_cores\": 48*num_producer_nodes,\n",
    "                 \"project\": \"TG-MCB090174\",\n",
    "                 \"queue\": \"normal\",\n",
    "                 \"walltime\": 300,\n",
    "                 \"type\":\"dask\"\n",
    "            }\n",
    "        dask_pilot = pilot.streaming.PilotComputeService.create_pilot(dask_pilot_description)\n",
    "        dask_pilot.wait()\n",
    "        \n",
    "        for num_broker_nodes in [2,4]:\n",
    "            #for application in [\"kmeans-5000\", \"kmeansstatic-5000\", \"kmeansstatic-10000\", \"kmeansstatic-20000\", \"light\"]:\n",
    "            for application in [\"kmeans-5000\", \"kmeansstatic-5000\",\"light\"]:    \n",
    "                kafka_pilot_description1 = {\n",
    "                    \"resource\":\"slurm+ssh://login1.wrangler.tacc.utexas.edu\",\n",
    "                    \"working_directory\": os.path.join('/work/01131/tg804093/wrangler/', \"work\"),\n",
    "                    \"number_cores\": 48*num_broker_nodes,\n",
    "                    \"project\": \"TG-MCB090174\",\n",
    "                    \"queue\": \"normal\",\n",
    "                    \"walltime\": 300,\n",
    "                    \"type\":\"kafka\"\n",
    "                }\n",
    "                kafka_pilot = pilot.streaming.PilotComputeService.create_pilot(kafka_pilot_description1)\n",
    "                kafka_pilot.wait()\n",
    "                               \n",
    "                \n",
    "                time.sleep(5)\n",
    "                for number_parallel_tasks in [8]:\n",
    "                    number_parallel_tasks = num_producer_nodes*number_parallel_tasks\n",
    "                    number_points_per_message = 5000\n",
    "                    if application.startswith(\"kmeans\"):\n",
    "                        number_points_per_message = int(application.split(\"-\")[1])\n",
    "                        application = application.split(\"-\")[0]\n",
    "                    \n",
    "                    print \"Run Application: %s, Number Messages: %d\"%(application, number_points_per_message)\n",
    "                    run_id = str(uuid.uuid1())\n",
    "                    miniapp=mass.kafka.MiniApp(\n",
    "                                                 dask_scheduler=dask_pilot.get_details()['master_url'],\n",
    "                                                 kafka_zk_hosts=kafka_pilot.get_details()[\"master_url\"],\n",
    "                                                 number_parallel_tasks=number_parallel_tasks,\n",
    "                                                 number_clusters=192, # kmeans\n",
    "                                                 number_points_per_cluster=52084, # kmeans\n",
    "                                                 number_points_per_message=number_points_per_message, # kmeans\n",
    "                                                 number_dim=3, # kmeans\n",
    "                                                 number_messages=6400, # light\n",
    "                                                 number_produces=8,\n",
    "                                                 number_partitions=num_broker_nodes*12,\n",
    "                                                 topic_name=\"test-\"+run_id,\n",
    "                                                 application_type = application\n",
    "                                                )\n",
    "                    miniapp.run()\n",
    "                try:\n",
    "                    kafka_pilot.cancel()\n",
    "                    \n",
    "                except:\n",
    "                    pass\n",
    "        try:\n",
    "            dask_pilot.cancel()\n",
    "            #dask_pilot2.cancel()\n",
    "            time.sleep(60)\n",
    "        except: \n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'master_url': 'tcp://c251-132:8786', 'web_ui_url': 'http://c251-132:8787'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dask_pilot.get_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import distributed\n",
    "c=distributed.Client(\"tcp://c251-132:8786\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.scheduler_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Mini App Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Kafka Broker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pykafka.KafkaClient(zookeeper_hosts=kafka_details[\"master_url\"])\n",
    "topic = client.topics['test']\n",
    "producer = topic.get_sync_producer()\n",
    "consumer = topic.get_simple_consumer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "number_total_points = 0\n",
    "number_dimensions = 0\n",
    "for i in range(100):\n",
    "    message = consumer.consume(block=False)\n",
    "    if message is not None:\n",
    "        data_np = np.array(ast.literal_eval(message.value))\n",
    "        num_points = data_np.shape[0]\n",
    "        number_dimensions = data_np.shape[1]\n",
    "        count =  count + 1\n",
    "        number_total_points = number_total_points + num_points\n",
    "    #print \"Consumed message: %d, Number Points: %d, Number Dimensions: %d\"%\\\n",
    "    #        (count, num_points, number_dimensions)   \n",
    "        \n",
    "print(\"Total Messages: %d, Total Points: %d, Number Dimensions: %d\"%(count, number_total_points, number_dimensions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
